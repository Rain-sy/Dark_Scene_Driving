{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLBhrncCyNhA"
      },
      "source": [
        "# Semantic Segmentation on BDD10k and Dark Zurich using the U-net ResNet-34 Model\n",
        "\n",
        "This notebook contains the code for fine-tuning the pre-trained U-net model on the BDD10k training images, and evaluating on Dark Zurich and BDD10k validation images.\n",
        "\n",
        "NOTE: I had to clear the cell outputs of this notebook in order to fit the file size constraints for uploading to github. However, you can view the cell outputs in my Google CoLab Notebook: https://colab.research.google.com/drive/1fe95l4P-zbXTtlqoaFwwRri-nlRx3SzN?usp=sharing\n",
        "\n",
        "NOTE: much of the code in this notebook was borrored or adapted from this repository: https://github.com/ronigold/sem-seg-bdd100k/tree/main\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i89aO_WNXxeU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkQQ4Dk0z3zU"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from fastai.vision.all import *\n",
        "import random\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-6_h-BsMcGm",
        "outputId": "92d834f2-a6be-4a4c-f25a-afaa53e369ca"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw1iFECv0Ujf"
      },
      "outputs": [],
      "source": [
        "# Define the base paths\n",
        "BASE_PATH = Path('/content/drive/MyDrive/')\n",
        "INPUT_PATH = BASE_PATH/'bdd10k_images'\n",
        "TARGET_PATH = BASE_PATH/'bdd100k_seg_maps/labels/'\n",
        "\n",
        "# Define the pixel2class dict\n",
        "vocab_dict = {\n",
        "    0: \"road\",\n",
        "    1: \"sidewalk\",\n",
        "    2: \"building\",\n",
        "    3: \"wall\",\n",
        "    4: \"fence\",\n",
        "    5: \"pole\",\n",
        "    6: \"traffic light\",\n",
        "    7: \"traffic sign\",\n",
        "    8: \"vegetation\",\n",
        "    9: \"terrain\",\n",
        "    10: \"sky\",\n",
        "    11: \"person\",\n",
        "    12: \"rider\",\n",
        "    13: \"car\",\n",
        "    14: \"truck\",\n",
        "    15: \"bus\",\n",
        "    16: \"train\",\n",
        "    17: \"motorcycle\",\n",
        "    18: \"bicycle\",\n",
        "    19: \"unknown\"\n",
        "}\n",
        "\n",
        "# Training the UNET Model\n",
        "def find_unique_classes_in_masks(mask_dir):\n",
        "    unique_classes = set()\n",
        "    for mask_path in mask_dir.iterdir():\n",
        "        if mask_path.is_file() and mask_path.suffix in ['.png', '.jpg']:  # Ensure it's an image file\n",
        "            mask = np.array(Image.open(mask_path))\n",
        "            unique_classes.update(np.unique(mask))\n",
        "    return unique_classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4QHSL8MIDyn",
        "outputId": "c143a0d6-fbfa-4c97-805d-6e269b5aff39"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing your validation masks\n",
        "mask_dir = TARGET_PATH/'val'\n",
        "\n",
        "# Find unique classes\n",
        "unique_classes = find_unique_classes_in_masks(mask_dir)\n",
        "\n",
        "print(f\"Unique classes found: {sorted(unique_classes)}\")\n",
        "print(f\"Total number of unique classes: {len(unique_classes)}\")\n",
        "assert(len(unique_classes) == len(vocab_dict))\n",
        "\n",
        "# Define the directory containing your training masks\n",
        "mask_dir = TARGET_PATH/'train'\n",
        "\n",
        "# Find unique classes\n",
        "unique_classes = find_unique_classes_in_masks(mask_dir)\n",
        "\n",
        "print(f\"Unique classes found: {sorted(unique_classes)}\")\n",
        "print(f\"Total number of unique classes: {len(unique_classes)}\")\n",
        "assert(len(unique_classes) == len(vocab_dict))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGYFbKANIHv_"
      },
      "outputs": [],
      "source": [
        "def get_adjusted_mask_file_path(x, mask_source_dir=None):\n",
        "    \"\"\"\n",
        "    Given an input image path, returns the corresponding mask with values adjusted.\n",
        "    All 255 values in the mask are changed to 19.\n",
        "\n",
        "    Args:\n",
        "    - x (Pathlib.Path): Path to the input image file.\n",
        "    - mask_source_dir (Pathlib.Path, optional): The absolute path to the directory\n",
        "                                                containing the corresponding masks.\n",
        "                                                If None, it infers the mask path\n",
        "                                                based on the existing TARGET_PATH logic for 'val'/'train'.\n",
        "\n",
        "    Returns:\n",
        "    - PIL.Image: The adjusted mask image.\n",
        "    \"\"\"\n",
        "    base = x.stem\n",
        "    mask_filename = base + \"_train_id.png\"\n",
        "\n",
        "    if mask_source_dir:\n",
        "        mask_path = mask_source_dir / mask_filename\n",
        "    else:\n",
        "        # Original logic for 'val' and 'train' subfolders\n",
        "        is_validation = \"val\" in str(x)\n",
        "        if is_validation:\n",
        "            mask_path = TARGET_PATH / \"val\" / mask_filename\n",
        "        else:\n",
        "            mask_path = TARGET_PATH / \"train\" / mask_filename\n",
        "\n",
        "    mask = np.array(Image.open(mask_path))\n",
        "    mask[mask == 255] = 19             # Remap unknown → class 19\n",
        "    return Image.fromarray(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "PpBk9Kp3KU1g",
        "outputId": "874f9851-6e06-4b01-fa93-163927a9ddaf"
      },
      "outputs": [],
      "source": [
        "def custom_splitter(file_path):\n",
        "    \"\"\"Custom splitter for DataBlock to separate training and validation datasets based on folder structure.\"\"\"\n",
        "    is_valid = 'val' in str(file_path)\n",
        "    return is_valid\n",
        "\n",
        "# Load the DataBlock\n",
        "segmentation_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=None)), # codes are the classes for segmentation\n",
        "                                   get_items=get_image_files,\n",
        "                                   splitter=FuncSplitter(custom_splitter),\n",
        "                                   get_y=get_adjusted_mask_file_path,\n",
        "                                   item_tfms=Resize(460),\n",
        "                                   batch_tfms=[*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)])\n",
        "\n",
        "# Load the DataLoaders\n",
        "dls = segmentation_datablock.dataloaders(INPUT_PATH, path=BASE_PATH, bs = 64, num_workers=11, pin_memory=True,\n",
        "    prefetch_factor=4)\n",
        "\n",
        "dls.show_batch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYDHBgl2TVJp",
        "outputId": "66302f3f-1471-45ef-c6f2-2f8cfc0220d6"
      },
      "outputs": [],
      "source": [
        "dls.train_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PALSo1GXTdtn",
        "outputId": "9a9a2fb9-700c-4f25-eed7-8953ef606864"
      },
      "outputs": [],
      "source": [
        "dls.valid_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBvXMy1eTYte",
        "outputId": "e268317f-aa03-4074-a720-65bc2592173f"
      },
      "outputs": [],
      "source": [
        "learn = unet_learner(dls, resnet34, n_out=len(unique_classes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGqXK4GnX16X"
      },
      "source": [
        "## Model Training (DO NOT RUN EVERYTIME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "nHGdcF0-TgYr",
        "outputId": "a9ebbc76-8d07-4507-9d16-c436a5883fa3"
      },
      "outputs": [],
      "source": [
        "\n",
        "lrs = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xYxbV9xTiQs",
        "outputId": "b22de25b-d63f-4e2e-c4dd-e12d1dd58208"
      },
      "outputs": [],
      "source": [
        "lrs.valley\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mKKc43FcTk94",
        "outputId": "0558f463-b01f-47a2-88ec-84db46547123"
      },
      "outputs": [],
      "source": [
        "learn.fine_tune(25, base_lr = lrs.valley, cbs=[ShowGraphCallback(), CSVLogger(fname='log.csv')])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tquwf67TWkUP"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "learn.save('unet_segmentation_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYXULv1LXOf8"
      },
      "source": [
        "## Re-Load Trained Model from File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8e5a913",
        "outputId": "dfc51f85-6813-48a9-c58a-8ca7601c1398"
      },
      "outputs": [],
      "source": [
        "# To load the model from the file\n",
        "learn.load('unet_segmentation_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6A9ILMjdTo0v",
        "outputId": "803750c7-74f0-4d89-a0e1-b817af24a96b"
      },
      "outputs": [],
      "source": [
        "\n",
        "learn.show_results(dl=dls.valid, max_n=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uXHaO4PeT3l8",
        "outputId": "6e398210-c2a4-4600-a04f-2af96393f21a"
      },
      "outputs": [],
      "source": [
        "# predict on validation data\n",
        "inputs, preds, targets, decoded_preds = learn.get_preds(dl=dls.valid, with_input=True, with_decoded = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95ziJedvqNQC"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFczIKmzqKWk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def iou(preds, targs, num_classes=20):\n",
        "    # Calculate Intersection over Union (IoU) for each class\n",
        "    ious = []\n",
        "    preds = preds.view(-1)\n",
        "    targs = targs.view(-1)\n",
        "\n",
        "    for cls in range(num_classes):  # Exclude the last class ('unknown')\n",
        "        pred_inds = preds == cls\n",
        "        target_inds = targs == cls\n",
        "        intersection = (pred_inds[target_inds]).long().sum().item()  # True positives\n",
        "        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection\n",
        "        if union > 0:\n",
        "            ious.append((cls, float(intersection) / float(max(union, 1))))\n",
        "        else:\n",
        "            ious.append((cls, float('nan')))\n",
        "    return ious\n",
        "\n",
        "def precision_recall(preds, targs, num_classes=20):\n",
        "    # Calculate precision and recall for each class\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    preds = preds.view(-1)\n",
        "    targs = targs.view(-1)\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = preds == cls\n",
        "        target_inds = targs == cls\n",
        "        tp = (pred_inds[target_inds]).long().sum().item()  # True positives\n",
        "        fp = (pred_inds[~target_inds]).long().sum().item()  # False positives\n",
        "        fn = (~pred_inds[target_inds]).long().sum().item()  # False negatives\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else float('nan')\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
        "\n",
        "        precisions.append((cls, precision))\n",
        "        recalls.append((cls, recall))\n",
        "\n",
        "    return precisions, recalls\n",
        "\n",
        "def calculate_pixel_accuracy(preds, targs):\n",
        "    # Renamed to avoid collision with fastai.metrics.accuracy\n",
        "    preds = preds.view(-1)\n",
        "    targs = targs.view(-1)\n",
        "    correct = (preds == targs).float().sum()\n",
        "    return correct / preds.shape[0]\n",
        "\n",
        "def evaluate_segmentation_model(preds, targs, vocab_dict):\n",
        "    num_classes = len(vocab_dict)\n",
        "    ious = iou(preds, targs, num_classes)\n",
        "    precisions, recalls = precision_recall(preds, targs, num_classes)\n",
        "    acc = calculate_pixel_accuracy(preds, targs).item() # Call the custom accuracy function\n",
        "\n",
        "    # Ensure targs is of integer type before calling bincount\n",
        "    targs_int = targs.view(-1).long()  # Cast targs to long to use with bincount\n",
        "    class_frequencies = targs_int.bincount(minlength=num_classes)\n",
        "    total_pixels = class_frequencies.sum().item()\n",
        "    class_percentages = (class_frequencies / total_pixels * 100).tolist()\n",
        "\n",
        "    # Compile metrics into a DataFrame for neat presentation\n",
        "    metrics = []\n",
        "    for cls in range(num_classes):\n",
        "        class_name = vocab_dict[cls]\n",
        "        metrics.append({\n",
        "            'Class': class_name,\n",
        "            'IoU': ious[cls][1],\n",
        "            'Precision': precisions[cls][1],\n",
        "            'Recall': recalls[cls][1],\n",
        "            'Percentage (%)': class_percentages[cls]\n",
        "        })\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics)\n",
        "    metrics_df.set_index('Class', inplace=True)\n",
        "    print(f\"Overall Accuracy: {acc:.4f}\")\n",
        "    return metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "KhG7luXbqU6f",
        "outputId": "ffce00d8-df80-4536-9704-32a19013ee05"
      },
      "outputs": [],
      "source": [
        "evaluation_df = evaluate_segmentation_model(decoded_preds, targets, vocab_dict)\n",
        "evaluation_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MVyZorEz3-5",
        "outputId": "16614fc2-d487-4a8a-da8f-855e949304c7"
      },
      "outputs": [],
      "source": [
        "# mean IOU\n",
        "evaluation_df['IoU'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn3IH6DdqIWx"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fu_SGAFWT4cf",
        "outputId": "dcb479a3-0da8-4358-be55-0f0b18ac9e42"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "BDD_COLORS = [\n",
        "    (128,  64, 128),  # road\n",
        "    (244,  35, 232),  # sidewalk\n",
        "    (70,   70,  70),  # building\n",
        "    (102, 102, 156),  # wall\n",
        "    (190, 153, 153),  # fence\n",
        "    (153, 153, 153),  # pole\n",
        "    (250, 170,  30),  # traffic light\n",
        "    (220, 220,   0),  # traffic sign\n",
        "    (107, 142,  35),  # vegetation\n",
        "    (152, 251, 152),  # terrain\n",
        "    (70,  130, 180),  # sky\n",
        "    (220,  20,  60),  # person\n",
        "    (255,   0,   0),  # rider\n",
        "    (0,     0, 142),  # car\n",
        "    (0,     0,  70),  # truck\n",
        "    (0,    60, 100),  # bus\n",
        "    (0,    80, 100),  # train\n",
        "    (0,     0, 230),  # motorcycle\n",
        "    (119,  11,  32),  # bicycle\n",
        "    (0,     0,   0),  # unknown\n",
        "]\n",
        "\n",
        "BDD_COLORS = np.array(BDD_COLORS) / 255.0\n",
        "\n",
        "cmap = ListedColormap(BDD_COLORS)\n",
        "\n",
        "def insert_line_breaks(text, max_chars_per_line=30):\n",
        "    words = text.split(', ')\n",
        "    lines = []\n",
        "    current_line = ''\n",
        "    for word in words:\n",
        "        if len(current_line) + len(word) + 2 > max_chars_per_line:\n",
        "            lines.append(current_line)\n",
        "            current_line = word\n",
        "        else:\n",
        "            if current_line:\n",
        "                current_line += ', ' + word\n",
        "            else:\n",
        "                current_line = word\n",
        "    lines.append(current_line)  # add the last line\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def denormalized_image(img):\n",
        "    img = img.float()\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    if torch.min(img) < 0:\n",
        "        img = img * std + mean  # Denormalize\n",
        "    img = img.permute(1, 2, 0)  # Rearrange channels for plotting\n",
        "    img = img.clamp(0, 1)  # Clamp values to ensure they are within [0, 1] range\n",
        "    return img\n",
        "\n",
        "def get_classes_from_mask(mask, vocab_dict):\n",
        "    unique_classes = torch.unique(mask).tolist()\n",
        "    class_names = [vocab_dict[c] for c in unique_classes if c in vocab_dict]\n",
        "    return ', '.join(class_names)\n",
        "\n",
        "def visualize_segmentation_batch(inputs, true_masks=None, pred_masks=None, vocab_dict=None, num_imgs=3):\n",
        "    # Determine the effective number of images to plot based on the shortest provided list/tensor\n",
        "    effective_num_imgs = len(inputs)\n",
        "    if true_masks is not None:\n",
        "        effective_num_imgs = min(effective_num_imgs, len(true_masks))\n",
        "    if pred_masks is not None:\n",
        "        effective_num_imgs = min(effective_num_imgs, len(pred_masks))\n",
        "\n",
        "    # The actual number of images to plot will be the minimum of the user-requested num_imgs\n",
        "    # and the calculated effective_num_imgs.\n",
        "    num_to_plot = min(num_imgs, effective_num_imgs)\n",
        "\n",
        "    print(f\"Plotting {num_to_plot} images.\") # Added for debugging\n",
        "\n",
        "    cols = 2 if true_masks is None else 3\n",
        "    fig, axes = plt.subplots(num_to_plot, cols, figsize=(20, 5*num_to_plot), squeeze=False)\n",
        "\n",
        "    for i in range(num_to_plot):\n",
        "        input_image = denormalized_image(inputs[i])\n",
        "\n",
        "        ax0 = axes[i][0]\n",
        "        ax0.imshow(input_image)\n",
        "        ax0.set_title('Input Image')\n",
        "        ax0.axis('off')\n",
        "\n",
        "        # Show true mask if provided\n",
        "        if true_masks is not None:\n",
        "            ax1 = axes[i][1]\n",
        "            true_mask = true_masks[i].cpu() # Move to CPU for plotting\n",
        "            ax1.imshow(true_mask, cmap=cmap, interpolation='nearest')\n",
        "            true_objects = get_classes_from_mask(true_mask, vocab_dict)\n",
        "            ax1.set_title(f'True Mask\\n{insert_line_breaks(true_objects)}')\n",
        "            ax1.axis('off')\n",
        "\n",
        "        # Show predicted mask if provided\n",
        "        if pred_masks is not None:\n",
        "            ax_pred = axes[i][1] if true_masks is None else axes[i][2]\n",
        "            pred_mask = pred_masks[i].cpu() # Move to CPU for plotting\n",
        "            ax_pred.imshow(pred_mask, cmap=cmap, interpolation='nearest')\n",
        "            predicted_objects = get_classes_from_mask(pred_mask, vocab_dict)\n",
        "            ax_pred.set_title(f'Predicted Mask\\n{insert_line_breaks(predicted_objects)}')\n",
        "            ax_pred.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_segmentation_batch(inputs, targets, decoded_preds, vocab_dict=vocab_dict, num_imgs=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RwtZEYf824S"
      },
      "source": [
        "# Evaluate specifically on the dark images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfaLY1yIbFCc"
      },
      "outputs": [],
      "source": [
        "# define a custom splitter to force all images into validation\n",
        "def custom_splitter(file_path):\n",
        "    \"\"\"Custom splitter for DataBlock to separate training and validation datasets based on folder structure.\"\"\"\n",
        "    is_valid = 'val' in str(file_path)\n",
        "    return is_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vTyDxbOe831"
      },
      "outputs": [],
      "source": [
        "#DARK_VAL_MASKS_DIR = BASE_PATH/'bdd100k_seg_maps_dark'\n",
        "def get_adjusted_mask_file_path(x, mask_source_dir=None):\n",
        "    \"\"\"\n",
        "    Given an input image path, returns the corresponding mask with values adjusted.\n",
        "    All 255 values in the mask are changed to 19.\n",
        "\n",
        "    Args:\n",
        "    - x (Pathlib.Path): Path to the input image file.\n",
        "    - mask_source_dir (Pathlib.Path, optional): The absolute path to the directory\n",
        "                                                containing the corresponding masks.\n",
        "                                                If None, it infers the mask path\n",
        "                                                based on the existing TARGET_PATH logic for 'val'/'train'.\n",
        "\n",
        "    Returns:\n",
        "    - PIL.Image: The adjusted mask image.\n",
        "    \"\"\"\n",
        "    base = x.stem\n",
        "    mask_filename = base + \"_train_id.png\"\n",
        "\n",
        "    if mask_source_dir:\n",
        "        mask_path = mask_source_dir / mask_filename\n",
        "    else:\n",
        "        # Original logic for 'val' and 'train' subfolders\n",
        "        is_validation = \"val\" in str(x)\n",
        "        if is_validation:\n",
        "            mask_path = TARGET_PATH / \"val\" / mask_filename\n",
        "        else:\n",
        "            mask_path = TARGET_PATH / \"train\" / mask_filename\n",
        "\n",
        "    mask = np.array(Image.open(mask_path))\n",
        "    mask[mask == 255] = 19             # Remap unknown → class 19\n",
        "    return Image.fromarray(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9GcO3l848zjl",
        "outputId": "033469c9-d15e-40e8-8b1e-0e674be544b0"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "# Define the directory containing the dark validation images\n",
        "DARK_VAL_IMAGES_DIR = BASE_PATH/'bdd10k_images_dark'\n",
        "# Define the directory containing the corresponding masks for dark validation images\n",
        "# Assuming these masks are in a new subfolder 'val_dark' under TARGET_PATH\n",
        "DARK_VAL_MASKS_DIR = BASE_PATH/'bdd100k_seg_maps_dark'\n",
        "\n",
        "# Create a partial function for get_adjusted_mask_file_path for the dark validation set\n",
        "get_y_dark_val = partial(get_adjusted_mask_file_path, mask_source_dir=DARK_VAL_MASKS_DIR)\n",
        "\n",
        "# # Create a new DataBlock for the dark validation dataset\n",
        "# dark_val_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=None)),\n",
        "#                                get_items=get_image_files,\n",
        "#                                splitter=lambda x: [list(range(len(x)))],\n",
        "#                                get_y=get_y_dark_val,\n",
        "#                                item_tfms=Resize(224, method='crop'), # Changed to deterministic resize and center-crop to 224x224\n",
        "#                                batch_tfms=[Normalize.from_stats(*imagenet_stats)]) # Removed aug_transforms for deterministic validation\n",
        "\n",
        "dark_val_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=None)), # codes are the classes for segmentation\n",
        "                                   get_items=get_image_files,\n",
        "                                   splitter=FuncSplitter(custom_splitter),\n",
        "                                   get_y=get_y_dark_val,\n",
        "                                   item_tfms=Resize(460),\n",
        "                                   batch_tfms=[*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)])\n",
        "\n",
        "\n",
        "# Create DataLoaders for the dark validation set\n",
        "# Set num_workers=0 for deterministic evaluation to ensure consistent batch ordering\n",
        "dark_val_dls = dark_val_datablock.dataloaders(DARK_VAL_IMAGES_DIR, path=BASE_PATH, bs=4, num_workers=11, pin_memory=True, prefetch_factor=4)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8HXYuqBbhKh",
        "outputId": "b63ae88c-aa7c-4fd7-f4ca-b4b6681c9683"
      },
      "outputs": [],
      "source": [
        "# check that validation is full:\n",
        "print(dark_val_dls.train_ds)\n",
        "print(dark_val_dls.valid_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "q9wjJ4R1k9y0",
        "outputId": "dc1e2908-a265-4490-947c-2dd975d7a96b"
      },
      "outputs": [],
      "source": [
        "inputs_dark, preds_dark, targets_dark, decoded_dark = learn.get_preds(\n",
        "    dl=dark_val_dls.valid,\n",
        "    with_input=True,\n",
        "    with_decoded=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "TYrG6AkadLeo",
        "outputId": "aaaad202-f330-48d3-fb8c-e33e4f7db1cb"
      },
      "outputs": [],
      "source": [
        "evaluation_df = evaluate_segmentation_model(decoded_dark, targets_dark, vocab_dict)\n",
        "display(evaluation_df)\n",
        "# mean IOU\n",
        "# exclude the unknown class\n",
        "print(evaluation_df['IoU'][:-1].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aDH6Qt8kdTI5",
        "outputId": "66c44b01-6df1-4001-b6a6-ee9c506f4117"
      },
      "outputs": [],
      "source": [
        "visualize_segmentation_batch(inputs_dark, targets_dark, decoded_dark, vocab_dict=vocab_dict, num_imgs=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKH9tvdJhiDa"
      },
      "source": [
        "# Evaluate on enhanced dark images from bdd10k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "TLRevCVQhlgq",
        "outputId": "1185425a-a3f9-42f1-cd16-5acd2467316f"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "# Define the directory containing the dark validation images\n",
        "enhanced_IMAGES_DIR = BASE_PATH/'bdd10k_images_gamma'\n",
        "# Define the directory containing the corresponding masks for dark validation images\n",
        "DARK_VAL_MASKS_DIR = BASE_PATH/'bdd100k_seg_maps_dark'\n",
        "\n",
        "# Create a partial function for get_adjusted_mask_file_path for the dark validation set\n",
        "get_y_dark_val = partial(get_adjusted_mask_file_path, mask_source_dir=DARK_VAL_MASKS_DIR)\n",
        "\n",
        "# Create a new DataBlock for the dark validation dataset\n",
        "# dark_val_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=None)),\n",
        "#                                get_items=get_image_files,\n",
        "#                                splitter=lambda x: [list(range(len(x)))], # No splitting needed for a single eval set\n",
        "#                                get_y=get_y_dark_val,\n",
        "#                                item_tfms=Resize(224, method='crop'), # Changed to deterministic resize and center-crop to 224x224\n",
        "#                                batch_tfms=[Normalize.from_stats(*imagenet_stats)])\n",
        "\n",
        "dark_val_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=None)), # codes are the classes for segmentation\n",
        "                                   get_items=get_image_files,\n",
        "                                   splitter=FuncSplitter(custom_splitter),\n",
        "                                   get_y=get_y_dark_val,\n",
        "                                   item_tfms=Resize(460),\n",
        "                                   batch_tfms=[*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)])\n",
        "\n",
        "# Create DataLoaders for the dark validation set\n",
        "dark_val_dls_enhanced = dark_val_datablock.dataloaders(enhanced_IMAGES_DIR, path=BASE_PATH, bs=4, num_workers=11, pin_memory=True, prefetch_factor=4)\n",
        "\n",
        "inputs_dark_enhanced, preds_dark_enhanced, targets_dark_enhanced, decoded_dark_enhanced = learn.get_preds(\n",
        "    dl=dark_val_dls_enhanced.valid,\n",
        "    with_input=True,\n",
        "    with_decoded=True\n",
        ")\n",
        "\n",
        "# check dataset sizes\n",
        "print(dark_val_dls_enhanced.train_ds)\n",
        "print(dark_val_dls_enhanced.valid_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "zh90B9-ThtKC",
        "outputId": "6ab82484-70b2-4b54-e6d1-91631c3c0b64"
      },
      "outputs": [],
      "source": [
        "evaluation_df = evaluate_segmentation_model(decoded_dark_enhanced, targets_dark_enhanced, vocab_dict)\n",
        "display(evaluation_df)\n",
        "# Mean IOU\n",
        "# exclude the unknown class\n",
        "print(evaluation_df['IoU'][:-1].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rbs5Pqf2h2Sy",
        "outputId": "46a34fa4-09b9-47d2-c6db-cf4b81df38b2"
      },
      "outputs": [],
      "source": [
        "visualize_segmentation_batch(inputs_dark_enhanced, targets_dark_enhanced, decoded_dark_enhanced, vocab_dict=vocab_dict, num_imgs=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuII5L8hjfp5"
      },
      "source": [
        "# CLAHE enhanced\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "I9PMuR_hkXPA",
        "outputId": "faccbe52-605f-4121-a459-b3083f8e9239"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "# Define the directory containing the dark validation images\n",
        "enhanced_IMAGES_DIR = BASE_PATH/'bdd10k_images_dark_clahe'\n",
        "# Define the directory containing the corresponding masks for dark validation images\n",
        "DARK_VAL_MASKS_DIR = BASE_PATH/'bdd100k_seg_maps_dark'\n",
        "\n",
        "# Create a partial function for get_adjusted_mask_file_path for the dark validation set\n",
        "get_y_dark_val = partial(get_adjusted_mask_file_path, mask_source_dir=DARK_VAL_MASKS_DIR)\n",
        "\n",
        "# # Create a new DataBlock for the dark validation dataset\n",
        "# dark_val_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=None)),\n",
        "#                                get_items=get_image_files,\n",
        "#                                splitter=lambda x: [list(range(len(x)))], # No splitting needed for a single eval set\n",
        "#                                get_y=get_y_dark_val,\n",
        "#                                item_tfms=Resize(224, method='crop'), # Changed to deterministic resize and center-crop to 224x224\n",
        "#                                batch_tfms=[Normalize.from_stats(*imagenet_stats)])\n",
        "\n",
        "dark_val_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=None)), # codes are the classes for segmentation\n",
        "                                   get_items=get_image_files,\n",
        "                                   splitter=FuncSplitter(custom_splitter),\n",
        "                                   get_y=get_y_dark_val,\n",
        "                                   item_tfms=Resize(460),\n",
        "                                   batch_tfms=[*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)])\n",
        "\n",
        "\n",
        "# Create DataLoaders for the dark validation set\n",
        "dark_val_dls_enhanced = dark_val_datablock.dataloaders(enhanced_IMAGES_DIR, path=BASE_PATH, bs=4, num_workers=11, pin_memory=True, prefetch_factor=4)\n",
        "\n",
        "inputs_dark_enhanced, preds_dark_enhanced, targets_dark_enhanced, decoded_dark_enhanced = learn.get_preds(\n",
        "    dl=dark_val_dls_enhanced.valid,\n",
        "    with_input=True,\n",
        "    with_decoded=True\n",
        ")\n",
        "\n",
        "print(dark_val_dls_enhanced.train_ds)\n",
        "print(dark_val_dls_enhanced.valid_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "0NdBKyDKkn7j",
        "outputId": "bb8d91cd-f81a-43dc-b25a-fbcd5c90ced5"
      },
      "outputs": [],
      "source": [
        "evaluation_df = evaluate_segmentation_model(decoded_dark_enhanced, targets_dark_enhanced, vocab_dict)\n",
        "display(evaluation_df)\n",
        "# Mean IOU\n",
        "print(evaluation_df['IoU'][:-1].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs3kpBV1e4mc"
      },
      "outputs": [],
      "source": [
        "visualize_segmentation_batch(inputs_dark_enhanced, targets_dark_enhanced, decoded_dark_enhanced, vocab_dict=vocab_dict, num_imgs=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOHJXs3AxRLm"
      },
      "source": [
        "# Retinex-Enhanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "JgVK1Na_xTht",
        "outputId": "0072081e-d988-4c9e-f3cd-022b28d2d709"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "# Define the directory containing the dark validation images\n",
        "enhanced_IMAGES_DIR = BASE_PATH/'bdd10k_images_dark_retinex'\n",
        "# Define the directory containing the corresponding masks for dark validation images\n",
        "DARK_VAL_MASKS_DIR = BASE_PATH/'bdd100k_seg_maps_dark'\n",
        "\n",
        "# Create a partial function for get_adjusted_mask_file_path for the dark validation set\n",
        "get_y_dark_val = partial(get_adjusted_mask_file_path, mask_source_dir=DARK_VAL_MASKS_DIR)\n",
        "\n",
        "# # Create a new DataBlock for the dark validation dataset\n",
        "# dark_val_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=None)),\n",
        "#                                get_items=get_image_files,\n",
        "#                                splitter=lambda x: [list(range(len(x)))], # No splitting needed for a single eval set\n",
        "#                                get_y=get_y_dark_val,\n",
        "#                                item_tfms=Resize(224, method='crop'), # Changed to deterministic resize and center-crop to 224x224\n",
        "#                                batch_tfms=[Normalize.from_stats(*imagenet_stats)])\n",
        "\n",
        "dark_val_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=None)), # codes are the classes for segmentation\n",
        "                                   get_items=get_image_files,\n",
        "                                   splitter=FuncSplitter(custom_splitter),\n",
        "                                   get_y=get_y_dark_val,\n",
        "                                   item_tfms=Resize(460),\n",
        "                                   batch_tfms=[*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)])\n",
        "\n",
        "# Create DataLoaders for the dark validation set\n",
        "dark_val_dls_enhanced = dark_val_datablock.dataloaders(enhanced_IMAGES_DIR, path=BASE_PATH, bs=4, num_workers=11, pin_memory=True, prefetch_factor=4)\n",
        "\n",
        "inputs_dark_enhanced, preds_dark_enhanced, targets_dark_enhanced, decoded_dark_enhanced = learn.get_preds(\n",
        "    dl=dark_val_dls_enhanced.valid,\n",
        "    with_input=True,\n",
        "    with_decoded=True\n",
        ")\n",
        "\n",
        "print(dark_val_dls_enhanced.train_ds)\n",
        "print(dark_val_dls_enhanced.valid_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "PncLDE0cxYOd",
        "outputId": "eb416ded-cef1-4057-f17d-2ebe55c44467"
      },
      "outputs": [],
      "source": [
        "evaluation_df = evaluate_segmentation_model(decoded_dark_enhanced, targets_dark_enhanced, vocab_dict)\n",
        "display(evaluation_df)\n",
        "# Mean IOU\n",
        "print(evaluation_df['IoU'][:-1].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MjtIT7Evq5Cc",
        "outputId": "99464b3e-0aaa-4356-e933-992d064268d0"
      },
      "outputs": [],
      "source": [
        "visualize_segmentation_batch(inputs_dark_enhanced, targets_dark_enhanced, decoded_dark_enhanced, vocab_dict=vocab_dict, num_imgs=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZCyPyooH-v2"
      },
      "source": [
        "# Evaluation on Dark Zurich"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAQpHIFO5ubQ"
      },
      "source": [
        "## Attempt 1: Using LabelIds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT_ojM3-JU7F"
      },
      "outputs": [],
      "source": [
        "def get_adjusted_mask_file_path(x, mask_source_dir=None):\n",
        "    \"\"\"\n",
        "    Given an input image path, returns the corresponding mask with values adjusted.\n",
        "    All class IDs outside the 0-18 range (including 255) are remapped to 19 ('unknown').\n",
        "\n",
        "    Args:\n",
        "    - x (Pathlib.Path): Path to the input image file.\n",
        "    - mask_source_dir (Pathlib.Path, optional): The absolute path to the directory\n",
        "                                                containing the corresponding masks.\n",
        "\n",
        "    Returns:\n",
        "    - PIL.Image: The adjusted mask image.\n",
        "    \"\"\"\n",
        "    base = x.stem.removesuffix(\"_rgb_anon\")\n",
        "    mask_filename = base + \"_gt_labelIds.png\"\n",
        "\n",
        "    mask_path = mask_source_dir / mask_filename\n",
        "\n",
        "    # Explicitly check for mask existence for debugging\n",
        "    if not mask_path.exists():\n",
        "        print(f\"WARNING: Mask file not found for image: {x}. Expected at: {mask_path}\")\n",
        "        # Create a dummy black mask with the same dimensions as the input image\n",
        "        try:\n",
        "            img = Image.open(x)\n",
        "            # Ensure dummy mask is uint8 for compatibility with PIL.Image.fromarray\n",
        "            dummy_mask = np.zeros((img.size[1], img.size[0]), dtype=np.uint8)\n",
        "            return Image.fromarray(dummy_mask)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating dummy mask for {x}: {e}\")\n",
        "            raise FileNotFoundError(f\"Mask {mask_path} not found and failed to create dummy mask for {x}.\") from e\n",
        "\n",
        "    # Load mask as NumPy array, default dtype (usually uint8) is fine for Image.open\n",
        "    mask = np.array(Image.open(mask_path))\n",
        "    # print(\"HERE\")\n",
        "    # print(mask)\n",
        "    # Remap all values > 18 (including 255) to 19 (unknown)\n",
        "    # This operation can be performed on uint8 if all values are <= 255, which is the case here.\n",
        "    mask[mask > 18] = 19\n",
        "\n",
        "    # Return as PIL Image. It's now uint8, which PIL handles correctly.\n",
        "    return Image.fromarray(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "RJKxDZk3Y03w",
        "outputId": "2ec030c3-d1ea-4077-9ca3-c256458b1ea2"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing the dark validation images\n",
        "IMAGES_DIR = BASE_PATH/'dark_zurich_val'\n",
        "# Define the directory containing the corresponding masks for dark validation images\n",
        "DZ_MASKS_DIR = BASE_PATH/'dark_zurich_val_segmaps_labelIds'\n",
        "\n",
        "# Create a partial function for get_adjusted_mask_file_path for the dark validation set\n",
        "get_y_dz = partial(get_adjusted_mask_file_path, mask_source_dir=DZ_MASKS_DIR)\n",
        "\n",
        "# Create a new DataBlock for the dark validation dataset\n",
        "dz_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=list(vocab_dict.keys()))),\n",
        "                               get_items=get_image_files,\n",
        "                               #splitter=lambda x: [list(range(len(x)))], # No splitting needed for a single eval set\n",
        "                               get_y=get_y_dz,\n",
        "                               item_tfms=Resize(224, method='crop'), # Changed to deterministic resize and center-crop to 224x224\n",
        "                               batch_tfms=[Normalize.from_stats(*imagenet_stats)]) # Explicitly set cuda=False for debugging\n",
        "\n",
        "# Create DataLoaders for the dark validation set\n",
        "dz_dls = dz_datablock.dataloaders(IMAGES_DIR, path=BASE_PATH, bs=4, num_workers=0, pin_memory=True, prefetch_factor=4) # Explicitly set device to 'cpu'\n",
        "\n",
        "inputs_dz, preds_dz, targets_dz, decoded_dz = learn.get_preds(\n",
        "    dl=dz_dls,\n",
        "    with_input=True,\n",
        "    with_decoded=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vx3m1DSIA_G",
        "outputId": "a955d4e0-33d9-4f64-8b30-3c39773ca1da"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "# Define the directory containing the dark validation images\n",
        "IMAGES_DIR = BASE_PATH/'dark_zurich_val'\n",
        "# Define the directory containing the corresponding masks for dark validation images\n",
        "DZ_MASKS_DIR = BASE_PATH/'dark_zurich_val_segmaps_labelIds'\n",
        "\n",
        "# Create a partial function for get_adjusted_mask_file_path for the dark validation set\n",
        "get_y_dz = partial(get_adjusted_mask_file_path, mask_source_dir=DZ_MASKS_DIR)\n",
        "\n",
        "# Create a new DataBlock for the dark validation dataset\n",
        "dz_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=list(vocab_dict.keys()))),\n",
        "                               get_items=get_image_files,\n",
        "                               splitter=lambda x: [list(range(len(x)))], # No splitting needed for a single eval set\n",
        "                               get_y=get_y_dz,\n",
        "                               item_tfms=Resize(224, method='crop'), # Changed to deterministic resize and center-crop to 224x224\n",
        "                               batch_tfms=[Normalize.from_stats(*imagenet_stats, cuda=False)]) # Explicitly set cuda=False for debugging\n",
        "\n",
        "# Create DataLoaders for the dark validation set\n",
        "dz_dls = dz_datablock.dataloaders(IMAGES_DIR, path=BASE_PATH, bs=4, num_workers=0, pin_memory=True, prefetch_factor=4, device='cpu') # Explicitly set device to 'cpu'\n",
        "\n",
        "# Manual prediction loop to entirely bypass learn.get_preds reordering logic\n",
        "all_inputs = []\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "all_decoded = []\n",
        "\n",
        "learn.model.eval() # Set model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    for i, (xb, yb) in enumerate(dz_dls.train): # Changed dz_dls to dz_dls.train\n",
        "        print(f\"Processing batch {i}.\")\n",
        "        # xb, yb are already on CPU because dz_dls.device='cpu'\n",
        "        # Move them to the model's device (GPU) for inference\n",
        "        xb_gpu = xb.to(device)\n",
        "        yb_gpu = yb.to(device)\n",
        "\n",
        "        # Get raw model outputs\n",
        "        raw_preds = learn.model(xb_gpu)\n",
        "\n",
        "        # Get actual predictions (argmax for segmentation)\n",
        "        preds = raw_preds.argmax(dim=1)\n",
        "\n",
        "        # Decode predictions (if your model's loss_func has a specific decodes method, otherwise preds themselves)\n",
        "        # For segmentation, decoded_preds are usually just the preds if not using a custom decoder\n",
        "        # If learn.loss_func has a decodes method, you might do: decoded_preds = learn.loss_func.decodes(raw_preds)\n",
        "        decoded_preds = preds # Defaulting to preds as decoded_preds if no specific decoder is needed\n",
        "\n",
        "        all_inputs.append(xb.cpu()) # Store on CPU\n",
        "        all_preds.append(preds.cpu()) # Store on CPU\n",
        "        all_targets.append(yb.cpu()) # Store on CPU\n",
        "        all_decoded.append(decoded_preds.cpu()) # Store on CPU\n",
        "\n",
        "inputs_dz = torch.cat(all_inputs)\n",
        "preds_dz = torch.cat(all_preds)\n",
        "targets_dz = torch.cat(all_targets)\n",
        "decoded_dz = torch.cat(all_decoded)\n",
        "\n",
        "print(\"Manual prediction loop completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "cPUApe7vZPH0",
        "outputId": "1532a6fa-c333-48fe-8318-cb6c0cb8afbb"
      },
      "outputs": [],
      "source": [
        "evaluation_df = evaluate_segmentation_model(decoded_dz, targets_dz, vocab_dict)\n",
        "display(evaluation_df)\n",
        "# Mean IOU\n",
        "print(evaluation_df['IoU'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wZryXa_mZWw_",
        "outputId": "1a5c3522-6ed6-454e-9cca-f069ba403995"
      },
      "outputs": [],
      "source": [
        "visualize_segmentation_batch(inputs_dz, targets_dz, decoded_dz, vocab_dict=vocab_dict, num_imgs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHcoz21kZbqn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh5CxL5B6K_8"
      },
      "source": [
        "## Attempt 2: TrainIds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw1ZS0vd6Uno"
      },
      "outputs": [],
      "source": [
        "def get_adjusted_mask_file_path(x, mask_source_dir=None):\n",
        "    \"\"\"\n",
        "    Given an input image path, returns the corresponding mask with values adjusted.\n",
        "    All class IDs outside the 0-18 range (including 255) are remapped to 19 ('unknown').\n",
        "\n",
        "    Args:\n",
        "    - x (Pathlib.Path): Path to the input image file.\n",
        "    - mask_source_dir (Pathlib.Path, optional): The absolute path to the directory\n",
        "                                                containing the corresponding masks.\n",
        "\n",
        "    Returns:\n",
        "    - PIL.Image: The adjusted mask image.\n",
        "    \"\"\"\n",
        "    base = x.stem.removesuffix(\"_rgb_anon\")\n",
        "    mask_filename = base + \"_gt_labelTrainIds.png\"\n",
        "\n",
        "    mask_path = mask_source_dir / mask_filename\n",
        "\n",
        "    # Explicitly check for mask existence for debugging\n",
        "    if not mask_path.exists():\n",
        "        print(f\"WARNING: Mask file not found for image: {x}. Expected at: {mask_path}\")\n",
        "        # Create a dummy black mask with the same dimensions as the input image\n",
        "        try:\n",
        "            img = Image.open(x)\n",
        "            # Ensure dummy mask is uint8 for compatibility with PIL.Image.fromarray\n",
        "            dummy_mask = np.zeros((img.size[1], img.size[0]), dtype=np.uint8)\n",
        "            return Image.fromarray(dummy_mask)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating dummy mask for {x}: {e}\")\n",
        "            raise FileNotFoundError(f\"Mask {mask_path} not found and failed to create dummy mask for {x}.\") from e\n",
        "\n",
        "    # Load mask as NumPy array, default dtype (usually uint8) is fine for Image.open\n",
        "    mask = np.array(Image.open(mask_path))\n",
        "    # print(\"HERE\")\n",
        "    # print(mask)\n",
        "    # Remap all values > 18 (including 255) to 19 (unknown)\n",
        "    # This operation can be performed on uint8 if all values are <= 255, which is the case here.\n",
        "    mask[mask > 18] = 19\n",
        "\n",
        "    # Return as PIL Image. It's now uint8, which PIL handles correctly.\n",
        "    return Image.fromarray(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "yEegHk-n6NKz",
        "outputId": "b389d8c2-5461-489b-9876-b36b4ad6310b"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing the dark validation images\n",
        "IMAGES_DIR = BASE_PATH/'dark_zurich_images'\n",
        "# Define the directory containing the corresponding masks for dark validation images\n",
        "DZ_MASKS_DIR = BASE_PATH/'dark_zurich_val_segmaps_trainIds'\n",
        "\n",
        "# Create a partial function for get_adjusted_mask_file_path for the dark validation set\n",
        "get_y_dz = partial(get_adjusted_mask_file_path, mask_source_dir=DZ_MASKS_DIR)\n",
        "\n",
        "# Create a new DataBlock for the dark validation dataset\n",
        "dz_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=list(vocab_dict.keys()))),\n",
        "                               get_items=get_image_files,\n",
        "                               splitter=FuncSplitter(custom_splitter),\n",
        "                               get_y=get_y_dz,\n",
        "                              item_tfms=Resize(460),\n",
        "                              batch_tfms=[*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)])\n",
        "\n",
        "# Create DataLoaders for the dark validation set\n",
        "dz_dls = dz_datablock.dataloaders(IMAGES_DIR, path=BASE_PATH, bs=4, num_workers=11, pin_memory=True, prefetch_factor=4, shuffle_train=False) # Explicitly set device to 'cpu'\n",
        "\n",
        "print(dz_dls.train_ds)\n",
        "print(dz_dls.valid_ds)\n",
        "\n",
        "inputs_dz, preds_dz, targets_dz, decoded_dz = learn.get_preds(\n",
        "    dl=dz_dls.valid,\n",
        "    with_input=True,\n",
        "    with_decoded=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "wcHlRckX6n1U",
        "outputId": "d95f3337-76d2-4569-d447-b157259259bc"
      },
      "outputs": [],
      "source": [
        "evaluation_df = evaluate_segmentation_model(decoded_dz, targets_dz, vocab_dict)\n",
        "display(evaluation_df)\n",
        "# Mean IOU\n",
        "print(evaluation_df['IoU'][:-1].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ODAWeFnU6_na",
        "outputId": "977379d0-bd2e-4427-af5d-8b71940c16e2"
      },
      "outputs": [],
      "source": [
        "visualize_segmentation_batch(inputs_dz, targets_dz, decoded_dz, vocab_dict=vocab_dict, num_imgs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9qZxJdR7aeY"
      },
      "source": [
        "# Gamma enhancement on Dark Zurich"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ZtFTiIQK7d_P",
        "outputId": "aa933ea1-75c9-42e6-f88a-78488af71baf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the directory containing the dark validation images\n",
        "IMAGES_DIR = BASE_PATH/'dark_zurich_images_gamma'\n",
        "# Define the directory containing the corresponding masks for dark validation images\n",
        "DZ_MASKS_DIR = BASE_PATH/'dark_zurich_val_segmaps_trainIds'\n",
        "\n",
        "# Create a partial function for get_adjusted_mask_file_path for the dark validation set\n",
        "get_y_dz = partial(get_adjusted_mask_file_path, mask_source_dir=DZ_MASKS_DIR)\n",
        "\n",
        "# Create a new DataBlock for the dark validation dataset\n",
        "dz_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=list(vocab_dict.keys()))),\n",
        "                               get_items=get_image_files,\n",
        "                               splitter=FuncSplitter(custom_splitter),\n",
        "                               get_y=get_y_dz,\n",
        "                              item_tfms=Resize(460),\n",
        "                              batch_tfms=[*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)])\n",
        "\n",
        "# Create DataLoaders for the dark validation set\n",
        "dz_dls = dz_datablock.dataloaders(IMAGES_DIR, path=BASE_PATH, bs=4, num_workers=4, pin_memory=True, prefetch_factor=4) # Explicitly set device to 'cpu'\n",
        "\n",
        "print(dz_dls.train_ds)\n",
        "print(dz_dls.valid_ds)\n",
        "\n",
        "inputs_dz, preds_dz, targets_dz, decoded_dz = learn.get_preds(\n",
        "    dl=dz_dls.valid,\n",
        "    with_input=True,\n",
        "    with_decoded=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "zwvxcD_I9PXO",
        "outputId": "1b4ac430-34e5-49a0-d35b-6157f6a1e774"
      },
      "outputs": [],
      "source": [
        "evaluation_df = evaluate_segmentation_model(decoded_dz, targets_dz, vocab_dict)\n",
        "display(evaluation_df)\n",
        "# Mean IOU\n",
        "print(evaluation_df['IoU'][:-1].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YOphc_V8-e-B",
        "outputId": "e8d53a8c-a3aa-4778-b567-99f96c9b3f41"
      },
      "outputs": [],
      "source": [
        "visualize_segmentation_batch(inputs_dz, targets_dz, decoded_dz, vocab_dict=vocab_dict, num_imgs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d01P-rl87Rg"
      },
      "source": [
        "# Dark Zurich Clahe-enhanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "gawzhgrP8-d1",
        "outputId": "2000d503-23ea-4e17-abc9-7edff322de0f"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing the dark validation images\n",
        "IMAGES_DIR = BASE_PATH/'dark_zurich_images_clahe'\n",
        "# Define the directory containing the corresponding masks for dark validation images\n",
        "DZ_MASKS_DIR = BASE_PATH/'dark_zurich_val_segmaps_trainIds'\n",
        "\n",
        "# Create a partial function for get_adjusted_mask_file_path for the dark validation set\n",
        "get_y_dz = partial(get_adjusted_mask_file_path, mask_source_dir=DZ_MASKS_DIR)\n",
        "\n",
        "# Create a new DataBlock for the dark validation dataset\n",
        "dz_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=list(vocab_dict.keys()))),\n",
        "                               get_items=get_image_files,\n",
        "                               splitter=FuncSplitter(custom_splitter),\n",
        "                               get_y=get_y_dz,\n",
        "                              item_tfms=Resize(460),\n",
        "                              batch_tfms=[*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)])\n",
        "# Create DataLoaders for the dark validation set\n",
        "dz_dls = dz_datablock.dataloaders(IMAGES_DIR, path=BASE_PATH, bs=4, num_workers=11, pin_memory=True, prefetch_factor=4) # Explicitly set device to 'cpu'\n",
        "\n",
        "print(dz_dls.train_ds)\n",
        "print(dz_dls.valid_ds)\n",
        "\n",
        "inputs_dz, preds_dz, targets_dz, decoded_dz = learn.get_preds(\n",
        "    dl=dz_dls.valid,\n",
        "    with_input=True,\n",
        "    with_decoded=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "XAniPA819AjE",
        "outputId": "8b8a6f51-8c5c-4b8a-de3f-f1253aa06665"
      },
      "outputs": [],
      "source": [
        "evaluation_df = evaluate_segmentation_model(decoded_dz, targets_dz, vocab_dict)\n",
        "display(evaluation_df)\n",
        "# Mean IOU\n",
        "print(evaluation_df['IoU'][:-1].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ptcXOSJR-QYb",
        "outputId": "1ef25224-da3d-4af1-9caa-caa60a29d9ee"
      },
      "outputs": [],
      "source": [
        "visualize_segmentation_batch(inputs_dz, targets_dz, decoded_dz, vocab_dict=vocab_dict, num_imgs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dePh8kV99A5Z"
      },
      "source": [
        "# Dark Zurich Retinex Enhanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "wiCvNaJv9D1e",
        "outputId": "99ff2c47-764e-461b-d740-93201c0f38cc"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing the dark validation images\n",
        "IMAGES_DIR = BASE_PATH/'dark_zurich_images_retinex'\n",
        "# Define the directory containing the corresponding masks for dark validation images\n",
        "DZ_MASKS_DIR = BASE_PATH/'dark_zurich_val_segmaps_trainIds'\n",
        "\n",
        "# Create a partial function for get_adjusted_mask_file_path for the dark validation set\n",
        "get_y_dz = partial(get_adjusted_mask_file_path, mask_source_dir=DZ_MASKS_DIR)\n",
        "\n",
        "# # Create a new DataBlock for the dark validation dataset\n",
        "# dz_datablock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=list(vocab_dict.keys()))),\n",
        "#                                get_items=get_image_files,\n",
        "#                                #splitter=lambda x: [list(range(len(x)))], # No splitting needed for a single eval set\n",
        "#                                get_y=get_y_dz,\n",
        "#                                item_tfms=Resize(224, method='crop'), # Changed to deterministic resize and center-crop to 224x224\n",
        "#                                batch_tfms=[Normalize.from_stats(*imagenet_stats)]) # Explicitly set cuda=False for debugging\n",
        "\n",
        "# Create DataLoaders for the dark validation set\n",
        "dz_dls = dz_datablock.dataloaders(IMAGES_DIR, path=BASE_PATH, bs=4, num_workers=4, pin_memory=True, prefetch_factor=4) # Explicitly set device to 'cpu'\n",
        "\n",
        "print(dz_dls.train_ds)\n",
        "print(dz_dls.valid_ds)\n",
        "\n",
        "inputs_dz, preds_dz, targets_dz, decoded_dz = learn.get_preds(\n",
        "    dl=dz_dls.valid,\n",
        "    with_input=True,\n",
        "    with_decoded=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "bILUF79C9Gkk",
        "outputId": "e9016f88-f568-4088-8ca4-6074418a8815"
      },
      "outputs": [],
      "source": [
        "evaluation_df = evaluate_segmentation_model(decoded_dz, targets_dz, vocab_dict)\n",
        "display(evaluation_df)\n",
        "# Mean IOU\n",
        "print(evaluation_df['IoU'][:-1].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fjpd6Zzv_Nli",
        "outputId": "94dc3cfe-d096-45e6-e590-d74823750999"
      },
      "outputs": [],
      "source": [
        "visualize_segmentation_batch(inputs_dz, targets_dz, decoded_dz, vocab_dict=vocab_dict, num_imgs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrW2wOieZXRc"
      },
      "source": [
        "# Debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1480fc90"
      },
      "outputs": [],
      "source": [
        "# Manually inspect a processed mask from Dark Zurich\n",
        "\n",
        "# Get a list of image files from the Dark Zurich directory\n",
        "dz_image_files = get_image_files(IMAGES_DIR)\n",
        "\n",
        "if len(dz_image_files) > 0:\n",
        "    # Pick the first image for inspection\n",
        "    sample_image_path = dz_image_files[0]\n",
        "\n",
        "    # Apply the get_adjusted_mask_file_path function to get the processed mask\n",
        "    processed_mask_pil = get_y_dz(sample_image_path)\n",
        "    processed_mask_np = np.array(processed_mask_pil)\n",
        "\n",
        "    # Find unique classes in the processed mask\n",
        "    unique_classes_processed = np.unique(processed_mask_np)\n",
        "\n",
        "    print(f\"Sample Image: {sample_image_path.name}\")\n",
        "    print(f\"Unique classes in PROCESSED mask: {sorted(unique_classes_processed)}\")\n",
        "\n",
        "    # Check if all classes are within the expected range (0-19)\n",
        "    if all(0 <= cls <= 19 for cls in unique_classes_processed):\n",
        "        print(\"All unique classes in processed mask are within the 0-19 range. Remapping appears successful.\")\n",
        "    else:\n",
        "        print(\"WARNING: Some unique classes in processed mask are still outside the 0-19 range. Remapping issue persists.\")\n",
        "else:\n",
        "    print(f\"No image files found in {IMAGES_DIR}. Cannot perform mask inspection.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7hkFQH0LdRj"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing your validation masks\n",
        "mask_dir = BASE_PATH/'dark_zurich_val_segmaps_labelIds'\n",
        "\n",
        "# Find unique classes\n",
        "unique_classes = find_unique_classes_in_masks(mask_dir)\n",
        "\n",
        "print(f\"Unique classes found: {sorted(unique_classes)}\")\n",
        "print(f\"Total number of unique classes: {len(unique_classes)}\")\n",
        "assert(len(unique_classes) == len(vocab_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo3CfFf4X352"
      },
      "outputs": [],
      "source": [
        "# deleted section\n",
        "inputs_dz, preds_dz, targets_dz, decoded_dz = learn.get_preds(\n",
        "    dl=dz_dls,\n",
        "    with_input=True,\n",
        "    with_decoded=True\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
